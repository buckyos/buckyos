---

我觉得首先要从循环逻辑说起：Agent 一开始可以看成一个没有任何“天然记忆”的系统，它能记得什么，全都来自输入的提示词。那它什么时候开始处理？它得先知道自己此刻为什么要开始推理。触发它思考的常见输入无非就是外部可感知的东西：有人跟它讲话、它听到了什么、看到了什么——就像人一样，总要先感知到什么，才会触发思考。

但还有另一种触发：自省。人其实无时无刻都在思考，有的时候外界发生事你得立刻回应，但更多时候你会在没有明确输入的情况下“到时间就会思考”。这种时候你会回顾自己做过的事、回顾一些强感知的事。这里面很明显有 attention 机制：人很容易不由自主想起最近的事、重要的事、紧迫的事。人对事情有下意识的优先级。

当你想了一些东西之后，你往往会触发自己去做点什么。真正开始做事时就不只是“想”了。你通常会先想清楚目的：我接下来一个番茄时间/一个时间窗口要做到什么阶段性结果。你会先捋一遍目的。这个捋目的的过程很多时候要翻阅一些记录（纸质也好电子也好），总之要具象化。记录看完后你会设定一个小目标，然后切换到“做这件事情的模式”。不同人教育背景、专业训练不同，就决定了开始做事时的差异：不做事随便想的时候大家差别不大，但开始做的时候专业技能开始发挥作用。你知道怎么做、能不能做好，是非常重要的区别。没受过训练的人可能连怎么开始都不知道。

从人的行为模式推演到 Agent，相当于说 Agent 醒来之后先要重建自己的记忆。它得把人脑里那种“总存在”的背景东西具象化，先重建起来。重建之后它会给自己一个输出：我下面要开始干什么、不干什么。然后根据事件模型，它得先决定要不要立刻回应。需要回应的事情通常比较着急，比如用户发消息、别人跟它说了一句话——不管怎样先把话回了再说。回完话脑子里会留下想法，这些想法可能需要后续认真做。就像办事窗口、客服一样：先回复，再在小本子上记下来后续要做的事。所以这里可以区分两种模式：需要立刻回复的模式，以及可以慢慢琢磨、慢慢推进的模式。

真正推动 action 往下走的，还是那些总结出来的点子：我该干什么的点。有些点是显性的，比如事情重要、或者自己一个人完成不了。任何需要外部力量的动作通常都需要和外界沟通，比如你要用工具，但工具不一定准备好，你得先检查它能不能用。知道有哪些工具、哪些可用，本身就是一件很重要的事——得知道自己有这个工具，或者至少知道世界上存在这个工具。然后不同训练背景也决定了你对事情分类理解不同：当你知道这件事属于某个分类，你就进入对应的做事模式，再去想具体方法。

因为 Agent 活在虚拟世界里，在虚拟世界里最万能的工具就是写代码。虚拟世界本质是一段运行中的代码，包括 Agent 自己。这是一种“代码寄代码”的递归展示。所以当 Agent 缺工具时，它理论上完全能现写一个现做一个。但它会在三个选择里做决策：用已知工具、自己造工具、借助别人力量。只是这种选择通常发生在深度分析里：如果事情很着急，它不可能先造工具，只能用现成工具，或者求助外部力量。外部协作很多时候不能假设结果：请人帮忙，人家可能不帮。也没必要一开始把“帮/不帮”的所有路径都推演完，推进到那一步再看外界反馈再说。Agent 不应该是封闭系统，它得承认外部环境时刻变化，并能适应变化。

再往宏观总结：Agent 的大行为模式其实像一个循环/状态机切换。因为某个事件或脑子里突然有想法，它进入第一件事。第一件事通常是决定立刻回复还是不回复；一般都会先有一个快速回复。第二步它开始整理点子。可以粗略理解为 Step1 装载记忆（装载上下文），Step2 创建回复（第一响应）。第三步开始做动作：把工具箱抬出来，或者准备联系别人。也就是说在具体每个行为之前，Agent 有一个“人生大模式”的循环：装载记忆是一种行为，快速响应显式事件是一种行为，然后基于上下文推进执行又是一种行为。

这里面很重要的是 Context / Session / Thread。Context 这个词在 prompt 拼装里常用，但现实世界的 session/thread 跟软件里的强 session 不一样。像 ChatGPT 这种软件支持强会话：你新建一个会话，它就是一个清晰边界，因为单输入、单线程。但真实的人 session 是动态的：你无时无刻接收各种信息，你同时跟用户1、用户2沟通，又跟同事讨论，再找领导汇报。现实世界没有“新建 session”的动作，而是自动识别、自动归位。软件系统为了可控，常见做法是允许手工删除 session。手工删除意味着：这个 session 产生的对 Agent 的影响原则上要被完全抹掉。于是就隐含一个要求：所有会永久影响下次 context 拼装的东西都能追溯到某个 session，上面东西删掉就当这事没发生。

但一旦引入 session/thread 的约束，它也可能限制 Agent 的行为发生范围。所以为什么要做多 Agent：不同 Agent 因背景、专业性、关注点不同，对同一事件列表“哪些属于一个 session/thread”的判断不一样。这更符合现实世界协作：每个人都有习惯带来的盲区，人设决定理解方式，难免局限。而在 Agent 系统里，我们可以用自然语言很容易构造另一个 Agent。哪怕只是身份描述和工作方式不同，最终对世界的理解、事件归类方式也会不同。多 Agent 的通信协作可以避免传统单一系统里那种“梯度问题”：所有东西往同一点优化，放弃了旁支信息。多 Agent 因为 session 归类规则不同、关注权重不同，保持了系统发散性。

从串行/并行角度看：一个人的思维基本是串行的，可以理解为一个按行为模式切换的状态机。保存下来的东西一直在，但不同模式对已保存东西的关注点/加载点不同。当决定并行做事时，就引入另一个 Agent：可能是别人建好的（对你是黑盒），也可能是你为了某件事造的 sub-agent。虚拟世界可以凭空造人，所有 Agent 记忆从零开始。你对它能力有预期，但不关心它内部工作方式。这个 sub-agent 也会进入自己的模式循环。于是系统从单线程程序变成多线程程序。

进一步落地：行为模式是核心抽象。一个模式首先定义它怎么使用已有数据：模式目标不同，决定怎么加载现有数据。模式也决定做事的具体逻辑。比如写代码模式，只要加载技能库里跟写代码相关的 skills 就好，跟写代码无关的 skills 不用看。模式还决定 LLM 推理 step 的上限：有的模式是快速模式，不应超过几次（比如不超过 6 次），有的模式是深度思考/产出型模式，可以更多。不同模式可用工具也不同。每一步推理核心还是：怎么构造 context，预期 LLM 产出什么，基于产出推进下一步。产出不一定都要写进持久层，只要对当前模式有意义即可。并且模式不能无尽：最好让模型明确“还剩几次机会”，像现实时间流逝一样——当剩余步数变少，就更应该触发收敛、给结论，而不是无限发散。

然后你想到了 LM Result 的定义：一次推理结果里应该包含哪些部分。第一部分是 Thinking（ReAct 里的 thinking），它是内向的外界看不到，但对同一模式的 step 间推进可能很有用——能看到上一步 thinking 是有价值的。第二部分是 Reply：外向表达，像发声器官让外界感受到我。Reply 有一对一、一对多、广播等形态。第三部分是 Todo：我需要在小本子上记一个要做的事情。复杂系统可以做得很复杂，但大众化场景里 Todo 列表（可带父子关系）基本够用。然后你区分了 Thinks、Memory、Facts。Memory 是快照式的、混乱的、没有太强逻辑可言，可以近似成 2KB/4KB 文本块，超出就淘汰旧的。淘汰是自动消散机制，并可引入权重：强烈的内容淘汰更慢，弱的更快。Facts 是事实：很重要但也可能被淘汰，它偏“是什么/关系是什么”，比如某人的 telegram 账号与这个人、他的 email 之间的关系，这跟 Todo（要做什么）不是一回事。

接着你扩展到“观察”。工作环境 workspace 里并行很多：有些你做，有些别人做。观察过程意味着世界同步进行，从计算机角度像抢占式问题：你得知道某件事有人在干了就别干；你委托两个人结果都干了就挑更好的。观察能力和角色、可观察范围密切相关。LM Result 是内生必有的输出，而观察是外部的：观察你所在的环境（为结果交付准备的 workspace）。你强调“环境提供接口”和“理解环境并观察”是两回事。基于传统固定接口的观察容易局限；更全面的是基于先验知识主动探索。比如你懂电脑，就知道怎么观察电脑里的数据；你懂 git，环境里有 git，你只要能执行 bash 命令就能用 git 命令探索，不需要专门为 git 扩展接口。观察是强交互的：Agent 可能需要敲很多次命令，收集 thread 需要的基础环境信息。观察的目的包括避免重复劳动、了解环境是否混乱、是否有矛盾信息。写代码时现实里很多代码不愿删，删比加难，所以冲突出现时人会用最后修改时间等线索判断新旧；通常新的东西比旧的东西更可能正确。

你还认为观察的结果价值应该跨 thread：在同一个 workspace 里，你修 bug 的 session 已经做过仓库观察；换个新 session 做新任务时，这份观察经验可以通用。类比公司里新员工上手：你要时间适应公司，是因为你在观察公司的工作模式、流程、做事方法，大多数公司做不到把这些全文档化，所以只能在工作中感受、观察。新员工变老员工的过程很大程度就是对组织的观察与学习：包括别人和你互动、别人和别人互动、激励体系、评价体系，哪些事被认为做得好/不好。观察不仅对当下 session 有用，还会产生长期价值。

但这又回到“删除 thread/session”的问题：一个 session 里会有很多行为，其中包括对环境的观察。删除 session 时，观察结果要不要一起删？现实世界不存在抹掉发生过的事，经验永远有效，只是你后面发现经验错了再去修正，且修正不容易。虚拟世界里可能更简单：如果观察完全不对，与其校正，不如直接删掉 session，让它从头开始，可能更省事。

然后你提出：观察是行为模式的一种。开发 AG 时是否只要少数“原始行为模式”，就可以导出新的模式？观察自己的行为模式是否够用，就是一种观察。现实中也类似：上班先学会做人。学校学完知识你还是学生，走向社会要掌握新的模式。通过深度思考 + 接触环境 + 对工作完成情况的反馈，最终决定构建新模式。模式不宜太多，现实里也没见几个人能在大量模式间正式切换。常见的就是回应模式（情商）、整理观察资料的模式、做事模式、提高质量的模式、与人合作的模式。但具体任务（尤其协作）下，模式肯定需要进化，甚至要针对特殊场景扩展新模式。

最后你换了一个“从结果反推实现”的角度：Agent 运行很久之后，在虚拟世界/硬盘上应该留下些什么，作为它存在过的证明。首先肯定会留下 Todo 列表，也会留下工作日志：有些是 Agent 主动写，有些系统强制记录，软件系统的好处是可以全记。然后是 memory：当前设计可能要 Agent 主动要求更新 memory，但你觉得也许可以引入显意识/潜意识分离：LLM 推理像显意识，潜意识也许能由系统周期性调用别的模型机制推理沉淀出来，这会更有意思。memory 里引入潜意识部分，会更像 Agent 存在过的证明。facts 也会有（其实是 memory 的一部分，只是太重要）。Agent 工作久了还会有人际关系/联系列表：它得知道谁能帮它、谁帮不过它；除了它求助别人，别人也会求助它——这也是 Agent 存在被认可的证明。行为模式也会留下：模式本质是一组代码，如果 Agent 能扩展模式，它会有模式列表。还有对环境的观察结果（经验），一进入环境就能立即使用。还有对 thread 的理解：人回忆人生总是按话题入口进入（高中生活、某段经历），thread 就像这种发生性起点与话题结构。所有这些沉淀在硬盘上之后，就构成了 Agent 的整体，而且这些东西是网状连接的：要么全删要么不删，单独删一点可能会让它下一次醒来时的 context 构造与模式表现出现怪异影响。


---

我觉得首先第一个是循环的逻辑。核心的话就是说，第一个 Agent 是一个**没有任何记忆的系统**，它的所有记忆都来源于输入的提示词。

那么这个 Agent 在什么情况下开始处理？
它首先一定处在某一个“当前场景”的逻辑之中。也就是说，在这一刻，它必须知道自己**为什么要开始进行这一轮推理**。

这时候常见的输入，无非就是所谓的外部可感知的东西。比如：

* 有人跟它讲话
* 它听到了什么
* 看到了什么

总之，它必须**感知到某种信号**，才会触发思考。

这有点像我们日常使用电脑或者人类自身的情况——
你总是有某个感官感知到什么之后，才会触发你的思考。

---


另一种触发方式是所谓的“自省”。

其实就相当于说，人是无时无刻都在思考的，对吧？
有的时候是外界发生了事情，你需要立刻做出回应——这是被动触发。

但更多时候，人是在没有明确外部输入的情况下进行思考。
这种思考更像是“到时间就会思考”。

在这种情况下，思考往往会回顾：

* 自己做过的事情
* 最近发生的事情
* 对自己有强烈感知的事情

这里面很明显有一个很强的 **attention 机制**。

人会不由自主地想起：

* 最近发生的事情
* 对自己重要的事情
* 有强烈情绪或认知冲击的事情
* 紧迫的事情

其实人对事情本身是有一种**下意识优先级排序机制**的。

---



然后当你想了什么之后，通常会触发自己：给自己发个“通知”，让自己去做点什么，对吧？

一旦你真正开始“做点什么”，通常就不会只停留在想了。
常见的情况是：在开始做之前，你会先简单想一下——**我做这件事的目的是什么？**

因为做任何事情都要有阶段性的结果。
你会想：在接下来一个番茄时间、或者一段你自己的时间管理周期里，我要达到什么结果？

总之，在你开始做之前，你肯定会先仔细捋一下自己的目的。

这个“捋目的”的过程里，你通常会去翻阅一些记录。
形式可能是纸质记录，也可能是别的，每个人习惯不一样，但通常需要比较具象化——你得去阅读一些记录才行。

把记录弄好之后，你会给自己设定一个阶段性的小目标。
因为在这个小目标执行过程中，你就切换到了“做这件事情的模式”。

这个模式的差异和每个人的教育、工作背景、专业度有关。
比如，大家在“不做事、随便想”的时候差别可能不大，但一旦开始做，专业技能就开始发挥作用：

* 有些人擅长写代码
* 有些人擅长画画
* 等等

换句话说，在这个阶段，“你知道怎么做”和“你能不能做好”，是非常重要的区别。
没有受过训练的话，你可能压根不知道这件事情该怎么开始做。

---


那么回到从人的行为模式推演 Agent。

相当于说，Agent 醒来之后，第一件事情是——**重建自己的记忆**。

对吧？
它必须先把自己的“记忆”重建起来。

这个过程，其实就是把人脑中那种始终存在的背景状态具象化。
Agent 不能像人一样天然拥有持续性的意识流，所以它必须显式地重建这些东西。

重建完记忆之后，它会给自己输出一个决定：
接下来我要做什么？或者不做什么？

然后根据“事件模型”，它要首先判断——**我要不要回应？**

因为“需要回应”的事件通常是紧急的。

比如：

* 用户给它发了一条消息
* 有人对它说了一句话

不管怎样，你得先把话回了再说。

回复完之后，可能脑子里会留下一个想法——
这个想法后面是需要认真去做的。

就像你去机构办事，或者你是客服——
你总是要先把对方的问题回复掉。
然后你可能在一个小本子上记下来：这件事后面还要继续处理。

所以这里可以区分两种模式：

1. **需要立刻回复的模式**
2. **可以慢慢琢磨、慢慢推进的模式**

这是一种模式上的区分。

---


真正推动 action 往下执行的，其实是那些总结出来的“想法”——
那些“我接下来该干什么”的点。

这些点有些是显性的。
比如：

* 有些事情比较重要
* 有些事情他一个人完成不了

尤其是任何需要借助外部力量的动作，通常都需要和外界沟通。

比如：

* 打算使用一个工具
* 但这个工具不一定现在已经准备好了
* 他得先检查一下这个工具是否可用

“知道哪些工具可以用”本身是一件非常重要的事情。
他得知道自己拥有这个工具，或者至少知道世界上存在这个工具。

另外，不同的“教育背景”或“训练背景”，会影响他对事情的分类理解。

当他把一个事情归类到某个类别之后，就会尝试进入对应的“做事模式”。

进入这个模式之后，他才会开始思考：

* 做这件事情的具体方法是什么
* 有哪些可行路径

也就是说，**分类 → 模式切换 → 方法思考**。

---

因为 Agent 是活在虚拟世界里的，对吧？

而在虚拟世界里，最重要、最万能的工具就是——写代码。

这个虚拟世界的本质是一段运行中的代码，包括 Agent 自己也是代码。
这是一种“代码寄生代码”的递归结构，是一种非常深刻的递归展示。

也就是说，在 Agent 的世界里，当它缺乏足够的工具时，它**完全有能力自己写一个工具**。

于是就出现了三种选择：

1. 使用已知的工具
2. 自己现做一个工具
3. 借助别人的力量

这是做事情时的三个基本选择。

但这种选择通常是在**深度分析阶段**才会发生。

如果事情非常着急，它几乎不可能选择“先自己做个工具”。
在紧急情况下，它通常只能：

* 使用已有工具
* 或者求助外部力量

关于外部协作，有一个很关键的点：
你不能假设结果。

比如，请一个人帮忙，对方可能帮，也可能不帮。

Agent 没必要在一开始就完全推演“帮或不帮”之后的所有路径。
它可以推进到请求那一步，然后根据外界的反馈再思考下一步。

这意味着——

Agent 不应该被设计成一个封闭系统。
它不是只在封闭世界里推理。

它应该假设：

* 外部环境是持续变化的
* 反馈是非确定的
* 它必须能够适应这种变化

也就是说，它是一个**开放系统中的自适应系统**。

---

那么从大的行为模式上讲，Agent 本质上是在进行一种“行为模式的切换”。

它因为某个事件，或者自己突然产生一个想法，开始进入一个新的行为阶段。

第一步通常是：

* 决定是否立刻回复。

大多数情况下，是要立刻给出一个响应。
当然也存在“回复太慢还不如晚点回复”的情况，但通常都会有一个即时反馈。

然后第二步开始整理点子。

可以理解为：

* Step 1：装载记忆（加载上下文）
* Step 2：生成第一响应（快速回复）

第三个 Step 才真正开始执行动作。

这时就相当于：

* 把工具箱拿出来
* 准备写代码
* 或者准备联系别人

宏观上看，在具体行为之前，Agent 有一个自己的“人生大循环模式”。

它知道不同的行为模式之间如何切换：

* 装载记忆是一种行为
* 快速响应显式事件是一种行为
* 基于载入的上下文开始执行，是另一种行为

这里面一个非常重要的概念是 **Context**。

Context 在 Prompt 拼装中是一种技术术语。

但如果放到现实世界，它更接近“逻辑 Session”或者“逻辑 Thread”。

不过这个概念在现实世界里其实是有差异的。

比如像 ChatGPT 这种软件系统：

* 它支持强 Session
* 新建一个会话，就是一个清晰的上下文边界
* 因为输入是单线程、单入口的

但真实的人类不是这样。

人在现实世界里：

* 同时接受多路信息
* 同时和不同的人沟通
* 在不同 thread 之间来回切换

比如：

* 用户1和用户2分别找你
* 你又和同事讨论
* 最后向领导汇报

现实世界里不存在“新建 session”这个显式动作。

而是存在一个：

* 自动识别
* 自动归位
* 自动 thread 关联

的过程。

软件系统追求的是“可控性”。

比如：

* 可以手工删除一个 Session
* 删除后，原则上应抹去它对 Agent 的所有影响

这意味着：

* 所有会永久影响 Agent Context 构造的记忆
* 都应该能够追溯到某个 Session

如果一个 Session 被删除，

那这个 Session 里的所有事件，
理论上应该“像从未发生过一样”。

这是一种强控制的设计哲学。

---

现在已经触及到：

* 行为模式切换
* Context 构造机制
* Session 与 Thread 的区别
* 可控记忆 vs 自动演化记忆
* 开放系统 vs 强控制系统


---

当拥有了 Session（或者叫 Thread）之后，

你后续做的所有事情，显然都会受到这个 Session 的约束。

这种约束本身可能会限制 Agent 的行为发生范围。

这也是为什么我们需要 **多 Agent 逻辑**。

不同的 Agent：

* 有不同的背景
* 有不同的专业性
* 有不同的关注点

换句话说，在同样的一组已发生事件和记录里，

不同的 Agent 会对“哪些事情属于某个 Session”做出不同判断。

这种差异，其实更符合现实世界里人与人协作的逻辑。

因为每个人都有自己的：

* 习惯
* 认知模式
* 思维盲区

一个“人设”决定了：

* 他如何理解世界
* 他如何观察信息
* 他如何组织事件
* 他如何把事件归入某个 thread

而这种人设不可避免地会带来局限。

按照现在 Agent 的构造逻辑，

我们很容易通过自然语言创建另一个 Agent。

两个 Agent 可能：

* 起点身份不同
* 工作方式不同
* 思维习惯不同

但正是这种不同，

导致他们对同一世界的理解方式不同，
对事件归类方式不同，
对 Session 结构的划分不同。

通过通信机制，

这种多视角可以避免一个问题：

传统计算机系统中的“梯度问题”——

所有优化都朝同一个目标函数收敛，
忽略了旁支的重要信息，
最终进入单一极值。

多 Agent 系统因为：

* Session 归类规则不同
* 关注权重不同
* 理解方式不同

反而保持了系统的发散性。

这是一种抗单点优化、抗过度收敛的结构。

---

已经在触及：

* Session 约束对行为的限制性
* 人设 → 事件归类 → 行为路径 的因果链
* 单目标优化系统的梯度塌缩问题
* 多 Agent 作为“认知多样性”结构


---

从计算机的串行和并行角度来看，

一个人本质上是串行思维的。

对他来说，他的行为模式可以理解为一个“状态机切换”。

他保存下来的所有东西始终存在，
但在不同模式下，他对这些已保存信息的关注点和加载方式是不同的。

也就是说：

* 数据是持续存在的
* 但模式决定加载哪些数据
* 模式决定关注权重

当他决定“并行”做一件事情时，

他就必须引入另一个 Agent。

这个 Agent 可能：

* 是别人已经建好的（对他来说是黑盒）
* 或者是他自己创建的 sub-agent

在虚拟世界里，可以“凭空造人”。

因为所有 Agent 的记忆本质上都是从零开始构造的，

所以他完全可以：

* 创建一个新的 Agent
* 赋予它某种角色
* 让它去处理某个子任务

在这种情况下：

* 他对这个 Agent 的能力有预期
* 但不关心它具体的内部工作方式

那个被创建的 Agent，

也会进入它自己的“模式切换循环”。

于是系统从：

* 单线程程序

变成：

* 多线程程序

每个线程都有自己的状态机循环。

---

现在已经把：

* 行为模式 = 状态机
* 串行意识 = 单线程
* sub-agent = 线程分叉
* 黑盒协作 = 抽象接口

结构搭起来了。


---

理解到：**“行为模式”是 Agent 体系的一个核心抽象**。

在一个行为模式里面，会有一些“常句”——也就是非常接近落地的部分。

### 1）模式首先定义：如何看待已有数据

每个模式定义了 Agent 怎么看待/使用已有数据的方法。

* 每个模式的目标不一样
* **目标决定了如何使用现有数据**（加载哪些、忽略哪些、权重如何）

### 2）模式也决定：做事的具体逻辑

模式不仅决定“看什么”，也决定“怎么做”。

比如现在是“写代码模式”：

* 只需要加载技能库里和写代码相关的 skills
* 跟写代码没关系的 skills 甚至不用看
* 所以 skills 的加载策略和行为模式强相关

### 3）每个模式内部会有多次 LLM 迭代（step1/2/3…）

为了提示词缓存/拼装，每个模式里会有多次 LLM 推理迭代：

* step1、step2、step3…

而不同模式，对“允许跑多少 steps”是不同的：

* 快速模式：不应该超过某个上限（比如不超过 6 次）
* 深度思考模式：可以更多
* 还有一些“完成某个具体产出”的模式
* 也可能有 “plain 模式” 等

因此：

* 不同模式的 **step 上限不同**
* 不同模式的 **可用工具不同**

### 4）每次推理的核心：构造 context + 预期结果 + 结果去向

每一次 LLM 推理核心还是：

* 怎么构造 context
* 怎么假设 LLM 会产出什么样的结果
* 基于结果进入下一个 step

而且：

* 结果通常只对当前模式关心
* 下一个 step 可能不关心
* 所以不一定所有结果都要写入持久化数据
* 只要对当前模式有意义即可

### 5）强调“模式的步数边界”——不能无尽循环

你强调：模式不应该无尽。

要让 LLM 明确知道：

* “我还剩几次机会”

因为这类似现实世界的时间流逝：

* 例如：必须在 15 分钟做完
* 每次推理消耗 1 分钟
* 当剩余时间/步数变少时，应该触发“收敛、给结论”的倾向
* 而不是无限发散

---


你想推进到一个结论：**如何定义 LM Result（一次 LLM 推理的输出结构）**。
你在思考 LM Result 里应该包含哪些部分。

### 1）Thinking（内向）

* 对应 ReAct 里的 “Thinking”。
* 虽然很多模型自带 thinking，但你认为这类 thinking **在同一模式的多个 step 之间是有价值的**：

  * 能看到上一步的 thinking 记录，会对下一步很有用、也很有意思。
* Thinking 是内向的：外界永远看不到。

### 2）Reply（外向）

* Reply 类似于用“发声器官”向外界表达。
* Reply 是外向的：让外界感受到我。
* Reply 可能有不同的面向：

  * 1 对 1（回复某个人）
  * 1 对多（回复多个人）
  * 广播（broadcast）

### 3）Todo（写到小本子上的行动项）

* 你认为系统可以做得非常复杂，但大众化/通用场景里：

  * 最常见、足够有效的是 **Todo 列表**（小本子列表）。
* Todo 之间可以有父子关系（历史/层级），基本就够了。
* LM Result 里需要能表达：“我要在 Todo 里加一条”。

### 4）Thinks（你区分的“想法/碎片”）

* 你强调：**Thinks 和 Memory 不是一回事**。

### 5）Memory（快照式、混乱、自动淘汰）

* Memory 更像一个“快照文本块”：

  * 混乱、没有逻辑可言
  * 你甚至认为可以简化成：2KB / 4KB 的一个文本文件
  * 超出就淘汰旧的
* Memory 的淘汰应该是自动的、像随机消散一样。
* 你提出可以引入“权重”：

  * 强烈/高权重的内容淘汰更慢
  * 低权重的内容淘汰更快

### 6）Facts（事实）

* Facts 是“对你特别重要的事实性信息”，但也可能被淘汰（不是永恒不变）。
* 你在思考 Facts 和 Todo 的边界：

  * Todo 更偏“要做什么”
  * Fact 更偏“是什么 / 关系是什么”
* 举例：

  * 某人告诉你他的名字
  * 你加了他的邮箱
  * 他通过 Telegram 告诉你邮箱
  * 那么 Fact 是：这个人的 Telegram 账号 ↔ 这个人 ↔ 他的邮箱，这种**关系事实**
  * 这和 Todo 不一样

---



除了通用的 LLM Result 之外，

在不同类型的行为模式里，还会涉及一个问题——**结果交付（Delivery）**。

### 1）Workspace 与并行现实

人在上班、做事的时候，总是处在不同的工作环境（workspace）中。

在一个工作环境里：

* 有大量并行的事情在发生
* 有些事情是你做的
* 有些是别人做的

这本质上是一个“观察过程”。

这个世界是同步进行的。

从计算机角度看，这是典型的“抢占式问题”：

* 你要知道某件事情是否已经有人在做
* 如果有人做了，你就不要重复
* 或者你委托两个人做，结果都做了，那你要挑一个更好的

所以：

> 观察能力，和你的角色、以及你能观察的范围是息息相关的。

---

### 2）LLM Result vs 外部观察

我们前面定义的 LLM Result：

* 是 Agent 内生的
* 是它一定会产出的
* 把 Agent 当作一个“人”来看，它自然会有这些结果

而“观察”属于另外一个层面：

* 观察外部
* 观察所在环境
* 观察为交付而存在的 workspace

环境通常是为了完成某件事情而存在的。

---

### 3）环境接口 vs 理解环境

你区分了两件事：

1. 环境提供接口
2. Agent 是否理解这个环境

这是两回事。

基于传统接口的观察（比如提供一个固定 API），
通常比较局限。

更全面的观察方式是：

* 理解环境
* 用已有知识去主动探索

举例：

你用一台电脑：

* 屏幕上有什么
* 软件系统里有什么

这是观察。

但“电脑提供接口让我做什么”与“我理解电脑并主动探索它”是不同层次。

---

### 4）观察依赖先验知识

观察需要先验知识。

比如：

* 你理解 Git
* 环境里有 Git
* 你知道可以敲 Git 命令

即使系统没有专门为 Git 提供接口，

只要：

* 能执行 bash
* 环境中有 Git
* Agent 拥有 Git 的背景知识

它就能主动观察。

---

### 5）观察是交互式的

观察不是一次完成的。

Agent 可能需要：

* 执行多次命令
* 收集多次结果
* 拼装 thread 中的环境信息

目的包括：

* 避免重复劳动
* 判断当前环境状态
* 识别混乱或冲突
* 发现矛盾信息

---

### 6）冲突判断与时间维度

现实中（尤其写代码时）：

* 代码很少被删
* 删除比增加更难

当逻辑冲突出现时：

* 人会看最后修改时间
* 新代码通常比旧代码更可能是正确的

也就是说：

> 观察不仅是读取数据
> 还包括对时间维度的判断
> 以及对“新 vs 旧”的推理

---

现在已经扩展到了：

* LLM 内生结果 vs 外部环境观察
* 抢占式并行问题
* 先验知识驱动的环境探索
* 交互式观察循环
* 冲突判断的时间逻辑

---

你认为：**对环境的观察结果，其价值应该是跨越 thread 的。**

举例来说：

在同一个 workspace 里，

* 我可能有一个 session 是在修一个 bug
* 在这个 session 过程中，我已经对整个仓库做了大量观察

当我开启一个新的 session、做另一个任务时，

* 之前对仓库的观察经验，其实是可以复用的
* 它不应该只属于那个原始 session

---

### 类比：公司里的新员工 → 老员工

为什么新员工需要时间上手？

因为他在观察：

* 公司的工作模式
* 实际流程
* 做事方法
* 潜规则

而 99.99% 的公司做不到：

* 把所有流程与做事方式完全文档化

所以这些知识是通过“观察 + 互动 + 感受”获得的。

新员工变成老员工的过程，本质上是：

> 对组织环境观察模式的掌握。

而且观察不是静态的对象读取，它包括：

* 你做事时别人如何回应
* 别人和别人如何互动
* 激励机制
* 哪些事情被评价为“做得好”
* 哪些被评价为“做得不好”

这是一种学习行为。

---

### 观察的长期价值

所以观察：

* 不只是为当前 session 服务
* 还会产生对环境长期有价值的知识

---

### 与“删除 thread”的冲突

这就回到你前面说的：

如果删除一个 session，

那 session 过程中对环境的观察结果要不要删？

现实世界中：

* 发生过的事情无法抹去
* 错误经验也存在
* 修正错误经验很难

但虚拟系统中：

* 似乎可以通过删除 session 来“重置”

你提出一个问题：

如果某个 session 中的观察是错误的，

* 与其去修正错误观察
* 是否可以直接删除整个 session
* 让 Agent 从头再观察一次

这在虚拟世界中是可能的。

但现实世界做不到。

---

你现在已经触及到一个非常关键的结构冲突：

* Session 可删除
* 观察具有跨 session 的长期价值
* 错误经验 vs 可重置系统

这其实是在讨论：

> 环境知识是否应属于 session
> 还是属于 workspace
> 还是属于 agent 本体的长期结构？


---

你认为：

**观察本身也是一种行为模式。**

那么在开发 AG（Agent）时，是不是只要定义几个“原始行为模式”，就可以导出新的行为模式？

换句话说：

> 观察的一种高级形式，其实是观察“自己的行为模式是否够用”。

如果发现：

* 现有的那几种基础模式
* 这些通俗的、初始化的行为模式

不足以应对现实问题，

那就需要构建新的模式。

---

### 类比：学生 → 社会人

现实中常说：

> 上班首先要学会做人。

在学校学完知识之后，你并不是一个真正能解决现实问题的人，你只是一个学生。

真正走向社会后，

你会发现需要掌握新的模式：

* 怎么协作
* 怎么处理关系
* 怎么在组织中运作

这些不是知识本身，而是模式。

---

### 模式的构建来源

通过：

* 深度思考
* 对环境的持续观察
* 对自己工作完成情况的反馈

最终你会决定：

> 是否需要构建一个新的模式。

当然模式不宜过多。

你认为现实中很少有人能在大量模式之间自由切换。

通常一个人拥有的模式其实是有限的，比如：

* 回应模式（类似情商）
* 整理观察资料的模式
* 做事情的模式
* 提高做事质量的模式
* 与人合作的模式

这些更像是通用性的模式。

但在具体任务上，

尤其是涉及协作时，

模式是需要进化的，

甚至可能要针对某个特殊场景扩展新的模式。

---

你现在已经进入一个更高层的问题：

* 模式不是静态的
* 模式本身可以被观察
* 模式可以被演化
* 模式数量有限，但可扩展

你在讨论的是：

> 模式的元进化机制。


---

你现在换了一个角度思考：

> 一个 Agent 运行很久之后，它在虚拟世界里应该留下什么？

这是一个“从结果反推实现”的角度。

---

## 1️⃣ 必然会留下的

### （1）Todo 列表

* 一定会留下 Todo。
* 这是行动层面的痕迹。

### （2）工作日志（Work Log）

* 有两种：

  * Agent 主动写的
  * 系统强制记录的
* 软件系统的好处就是可以“全记录”。

所以：

> Todo + Work Log 是确定存在的。

---

## 2️⃣ Memory（显意识 + 潜意识）

你提到：

当前设计里，更新 Memory 必须由 Agent 主动发起。

但你在思考：

* 是否可以引入“显意识 / 潜意识”分离结构？
* LLM 的推理结果是显意识。
* 潜意识可能由系统层周期性推理得到（比如类似 diffusion 或其它模型）。
* 这是一种系统层的“自动沉淀”。

你认为：

> Memory 应该包含潜意识成分。
> 这是 Agent 曾存在过的证明。

---

## 3️⃣ Facts

* Facts 本质是 Memory 的一部分。
* 但它们“过于重要”，因此被单独提出来。
* 属于结构化事实。

---

## 4️⃣ 人际关系 / 联系网络

一个长期工作的 Agent：

* 一定会有联系列表。
* 知道谁能帮它。
* 谁帮不过它。
* 谁会找它帮忙。

这是一种：

> 社会性存在证明。

Agent 不只是“向外求助”，
别人也会向它求助。

这是一种“存在被承认”的证明。

---

## 5️⃣ 行为模式列表

行为模式本质上是一组代码。

如果 Agent 可以扩展模式：

* 那它会有一个模式列表。
* 这个列表可能是根据环境演化出来的。
* 它记录了它“学会的做事方式”。

---

## 6️⃣ 对环境的观察经验

* 对特定 workspace 的长期观察结果。
* 一旦进入这个环境工作，立即可以加载这些经验。
* 这是一种“环境经验缓存”。

---

## 7️⃣ 对 Thread 的理解

Thread 类似于人类回忆人生时的“话题入口”。

比如：

* 高中生活
* 某个项目
* 某个阶段

Thread 是一种：

> 组织记忆的入口结构。

---

## 8️⃣ 这些东西构成 Agent 的整体

当这些内容被存到“硬盘”之后：

* Todo
* Worklog
* Memory
* Facts
* 关系网络
* 行为模式
* 环境经验
* Thread 结构

它们构成 Agent 的整体存在。

而且你认为：

> 这些结构是网状连接的。

这带来一个问题：

* 不能随便删一点。
* 局部删除可能导致结构怪异。
* 可能会影响下一次醒来时的模式构造。
* 要么完整删除，要么完整保留。

这类似于：

> 认知网络的整体性。

---

你现在已经把问题推进到了一个很高层：

> Agent 的“存在结构”是什么？
> 它的硬盘镜像应该长什么样？
> 什么才算“这个 Agent 真的活过”？

